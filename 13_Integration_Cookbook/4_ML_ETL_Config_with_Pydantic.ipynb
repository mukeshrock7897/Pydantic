{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  ML/ETL Config with Pydantic\n",
    "\n",
    "### ğŸ¯ Intent\n",
    "\n",
    "Use **Pydantic v2** to define configs & schemas for ML/ETL pipelines â†’ reproducible runs, clean inputs, early failures.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© Core Components\n",
    "\n",
    "1. **âš™ï¸ Settings Layer**\n",
    "\n",
    "   * Centralize env configs with `BaseSettings`.\n",
    "   * Typical: `DATA_DIR: DirectoryPath`, `SEED: int`, `TRACKING_URL: AnyUrl`.\n",
    "   * Load via `.env`, env vars, `secrets_dir`.\n",
    "\n",
    "2. **ğŸ§± Pipeline Config Models**\n",
    "\n",
    "   * Split configs per stage: `ExtractConfig`, `TransformConfig`, `TrainConfig`, `PredictConfig`.\n",
    "   * Nest inside `AppConfig`.\n",
    "\n",
    "3. **ğŸ“Š Dataset Row Schema**\n",
    "\n",
    "   * Define row-level `BaseModel` with type/range/regex checks.\n",
    "   * Validate batches via `TypeAdapter(list[RowModel])`.\n",
    "\n",
    "4. **ğŸ·ï¸ Column Contracts**\n",
    "\n",
    "   * Use `Enum` / `Literal` for feature names (avoid typos).\n",
    "   * Map raw â†’ canonical via `Field(alias=\"raw_col\")`.\n",
    "\n",
    "5. **ğŸ”¢ Hyperparams as Types**\n",
    "\n",
    "   * `confloat(ge=0, le=1)` â†’ learning rate.\n",
    "   * `conint(ge=1)` â†’ depth.\n",
    "   * Discriminated unions for algo-specific configs (`algo=\"xgb\" | \"rf\"`).\n",
    "\n",
    "6. **ğŸ“ Paths & I/O**\n",
    "\n",
    "   * Use `Path`, `FilePath`, `DirectoryPath`, `AnyUrl`.\n",
    "   * Optional `@field_validator` to check existence.\n",
    "\n",
    "7. **ğŸ§ª Data Quality Guards**\n",
    "\n",
    "   * `@model_validator(mode=\"after\")`: check `start <= end`, no nulls, unique keys.\n",
    "   * Raise `PydanticCustomError` with clear codes.\n",
    "\n",
    "8. **ğŸš€ Batch & Stream Validation**\n",
    "\n",
    "   * Batch â†’ one adapter per chunk (fast).\n",
    "   * Stream â†’ same schema for Kafka/SQS messages.\n",
    "\n",
    "9. **ğŸ“¤ Reproducibility**\n",
    "\n",
    "   * Save config dump (`model_dump_json`) + compute hash for lineage.\n",
    "   * Store with metrics/artifacts.\n",
    "\n",
    "10. **ğŸ§° Schema Docs**\n",
    "\n",
    "* Publish `model_json_schema(by_alias=True)` for team contracts.\n",
    "* Snapshot-test schema for stability.\n",
    "\n",
    "11. **ğŸ›¡ï¸ PII & Secrets**\n",
    "\n",
    "* Use `SecretStr` + `@field_serializer` to mask.\n",
    "* Reference sensitive data by ID, not raw values.\n",
    "\n",
    "12. **âš¡ Performance**\n",
    "\n",
    "* Batch > row-by-row validation.\n",
    "* Keep models flat; avoid deep nesting.\n",
    "* Use discriminators instead of wide unions.\n",
    "\n",
    "13. **ğŸ§ª Testing**\n",
    "\n",
    "* Unit-test configs (valid/invalid).\n",
    "* Snapshot dumps + schema.\n",
    "* Seed values for reproducibility.\n",
    "\n",
    "14. **ğŸ§­ Orchestration-Friendly**\n",
    "\n",
    "* Keep configs JSON/YAML serializable.\n",
    "* Pass typed configs through Airflow, Prefect, Cron jobs.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
